{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed Point Diffusion Models (FPDM)\n",
    "This notebook shows how to run the image sampling with FPDM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up\n",
    "We provide an environment.yml file that can be used to create a Conda environment. See how to install all required packages in `README.md`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "from contextlib import nullcontext\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "# Third-party imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torch import Tensor\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "from diffusers.models import AutoencoderKL\n",
    "from jaxtyping import Float, Shaped\n",
    "from tap import Tap\n",
    "from tqdm import trange\n",
    "from timm.models.vision_transformer import PatchEmbed, Attention, Mlp\n",
    "\n",
    "# Local module imports\n",
    "sys.path.append(\"..\")\n",
    "from diffusion import create_diffusion\n",
    "from download import find_model\n",
    "from models import DiT_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(Tap):\n",
    "    \"\"\"\n",
    "    A class to define and store hyperparameters and configurations for the demo.\n",
    "    \"\"\"\n",
    "\n",
    "    # File and directory paths.\n",
    "    output_dir: str = 'demo_samples'\n",
    "\n",
    "    # Dataset configuration.\n",
    "    dataset_name: str = \"imagenet256\"\n",
    "\n",
    "    # Model specific parameters.\n",
    "    model: str = \"DiT-XL/2\"\n",
    "    vae: str = \"mse\"\n",
    "    num_classes: int = 1000\n",
    "    image_size: int = 256\n",
    "    predict_v: bool = False\n",
    "    use_zero_terminal_snr: bool = False\n",
    "    unsupervised: bool = False\n",
    "    dino_supervised: bool = False\n",
    "    dino_supervised_dim: int = 768\n",
    "    flow: bool = False\n",
    "    debug: bool = False\n",
    "\n",
    "    # Fixed Point settings.\n",
    "    fixed_point: bool = False\n",
    "    fixed_point_pre_depth: int = 2\n",
    "    fixed_point_post_depth: int = 2\n",
    "    fixed_point_iters: Optional[int] = None\n",
    "    fixed_point_pre_post_timestep_conditioning: bool = False\n",
    "    fixed_point_reuse_solution: bool = False\n",
    "\n",
    "    # Sampling configuration.\n",
    "    ddim: bool = False\n",
    "    cfg_scale: float = 4.0\n",
    "    num_sampling_steps: int = 250\n",
    "    batch_size: int = 4\n",
    "    ckpt: str = '/work/xingjian/diff-deq-inference/pretrained/DiT-XL-2/checkpoints/0500000.pt'  # replace it with the Path to your checkpoint.\n",
    "    global_seed: int = 0\n",
    "\n",
    "    # Parallelization settings.\n",
    "    sample_index_start: int = 0\n",
    "    sample_index_end: Optional[int] = 32\n",
    "\n",
    "    def process_args(self):\n",
    "        \"\"\"\n",
    "        Method for additional argument processing and validation.\n",
    "        \"\"\"\n",
    "        # Debug mode configuration.\n",
    "        if self.debug:\n",
    "            self.log_with = 'tensorboard'\n",
    "            self.name = 'debug'\n",
    "\n",
    "        # Set default values and validate image size.\n",
    "        self.fixed_point_iters = self.fixed_point_iters or (28 - self.fixed_point_pre_depth - self.fixed_point_post_depth)\n",
    "        assert self.image_size % 8 == 0, \"Image size must be divisible by 8 (for the VAE encoder).\"\n",
    "        self.latent_size = self.H_lat = self.W_lat = self.image_size // 8\n",
    "\n",
    "        # Additional checks and validations.\n",
    "        if self.cfg_scale < 1.0:\n",
    "            raise ValueError(\"In almost all cases, cfg_scale should be >= 1.0\")\n",
    "        \n",
    "        if self.unsupervised:\n",
    "            assert self.cfg_scale == 1.0\n",
    "            self.num_classes = 1\n",
    "        elif self.dino_supervised:\n",
    "            raise NotImplementedError()\n",
    "        \n",
    "        if not Path(self.ckpt).is_file():\n",
    "            raise ValueError(self.ckpt)\n",
    "        \n",
    "        # Creating the output directory.\n",
    "        output_parent = Path(self.output_dir) / Path(self.ckpt).parent.parent.name\n",
    "        if self.debug:\n",
    "            output_dirname = 'debug'\n",
    "        else:\n",
    "            output_dirname = f'num_sampling_steps-{self.num_sampling_steps}--cfg_scale-{self.cfg_scale}'\n",
    "            if self.fixed_point:\n",
    "                output_dirname += f'--fixed_point_iters-{self.fixed_point_iters}--fixed_point_reuse_solution-{self.fixed_point_reuse_solution}--fixed_point_pptc-{self.fixed_point_pre_post_timestep_conditioning}'\n",
    "        if self.ddim:\n",
    "            output_dirname += f'--ddim'\n",
    "        self.output_dir = str(output_parent / output_dirname)\n",
    "        Path(self.output_dir).mkdir(exist_ok=True, parents=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(Tap):\n",
    "\n",
    "    # Paths\n",
    "    output_dir: str = 'samples'\n",
    "\n",
    "    # Dataset\n",
    "    dataset_name: str = \"imagenet256\"\n",
    "\n",
    "    # Model\n",
    "    model: str = \"DiT-XL/2\"\n",
    "    vae: str = \"mse\"\n",
    "    num_classes: int = 1000\n",
    "    image_size: int = 256\n",
    "    predict_v: bool = False\n",
    "    use_zero_terminal_snr: bool = False\n",
    "    unsupervised: bool = False\n",
    "    dino_supervised: bool = False\n",
    "    dino_supervised_dim: int = 768\n",
    "    flow: bool = False\n",
    "    debug: bool = False\n",
    "\n",
    "    # Fixed Point settings\n",
    "    fixed_point: bool = False\n",
    "    fixed_point_pre_depth: int = 2\n",
    "    fixed_point_post_depth: int = 2\n",
    "    fixed_point_iters: Optional[int] = None\n",
    "    fixed_point_pre_post_timestep_conditioning: bool = False\n",
    "    fixed_point_reuse_solution: bool = False\n",
    "\n",
    "    # Sampling\n",
    "    ddim: bool = False\n",
    "    cfg_scale: float = 4.0\n",
    "    num_sampling_steps: int = 250\n",
    "    batch_size: int = 4\n",
    "    ckpt: str = '/work/xingjian/diff-deq-inference/pretrained/DiT-XL-2/checkpoints/0500000.pt' # replace with path to checkpoint\n",
    "    global_seed: int = 0\n",
    "    \n",
    "    # Parallelization\n",
    "    sample_index_start: int = 0\n",
    "    sample_index_end: Optional[int] = 32\n",
    "\n",
    "    def process_args(self) -> None:\n",
    "        \"\"\"Additional argument processing\"\"\"\n",
    "        if self.debug:\n",
    "            self.log_with = 'tensorboard'\n",
    "            self.name = 'debug'\n",
    "\n",
    "        # Defaults\n",
    "        self.fixed_point_iters = self.fixed_point_iters or (28 - self.fixed_point_pre_depth - self.fixed_point_post_depth)\n",
    "        assert self.image_size % 8 == 0, \"Image size must be divisible by 8 (for the VAE encoder).\"\n",
    "        self.latent_size = self.H_lat = self.W_lat = self.image_size // 8\n",
    "        # Checks\n",
    "        if self.cfg_scale < 1.0:\n",
    "            raise ValueError(\"In almost all cases, cfg_scale should be >= 1.0\")\n",
    "        if self.unsupervised:\n",
    "            assert self.cfg_scale == 1.0\n",
    "            self.num_classes = 1\n",
    "        elif self.dino_supervised:\n",
    "            raise NotImplementedError()\n",
    "        if not Path(self.ckpt).is_file():\n",
    "            raise ValueError(self.ckpt)\n",
    "\n",
    "        # Create output directory\n",
    "        output_parent = Path(self.output_dir) / Path(self.ckpt).parent.parent.name\n",
    "        if self.debug:\n",
    "            output_dirname = 'debug'\n",
    "        else:\n",
    "            output_dirname = f'num_sampling_steps-{self.num_sampling_steps}--cfg_scale-{self.cfg_scale}'\n",
    "            if self.fixed_point:\n",
    "                output_dirname += f'--fixed_point_iters-{self.fixed_point_iters}--fixed_point_reuse_solution-{self.fixed_point_reuse_solution}--fixed_point_pptc-{self.fixed_point_pre_post_timestep_conditioning}'\n",
    "        if self.ddim:\n",
    "            output_dirname += f'--ddim'\n",
    "        self.output_dir = str(output_parent / output_dirname)\n",
    "        Path(self.output_dir).mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()\n",
    "args.process_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network architecture\n",
    "We modify the original DiT class to support fixed point blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from models import TimestepEmbedder, LabelEmbedder, DiTBlock, FinalLayer, get_2d_sincos_pos_embed\n",
    "class DiT(nn.Module):\n",
    "    \"\"\"\n",
    "    Diffusion model with a Transformer backbone. It includes methods for the forward pass\n",
    "    and initialization of weights. The model can operate in both standard and fixed-point modes.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size=32,\n",
    "        patch_size=2,\n",
    "        in_channels=4,\n",
    "        hidden_size=1152,\n",
    "        depth=28,\n",
    "        num_heads=16,\n",
    "        mlp_ratio=4.0,\n",
    "        class_dropout_prob=0.1,\n",
    "        num_classes=1000,\n",
    "        learn_sigma=True,\n",
    "        use_cfg_embedding: bool = True,\n",
    "        use_gradient_checkpointing: bool = True,\n",
    "        is_label_continuous: bool = False,\n",
    "\n",
    "        # below are Fixed Point-specific arguments.\n",
    "        fixed_point: bool = False,\n",
    "\n",
    "        # size\n",
    "        fixed_point_pre_depth: int = 1, \n",
    "        fixed_point_post_depth: int = 1, \n",
    "\n",
    "        # iteration counts\n",
    "        fixed_point_no_grad_min_iters: int = 0, \n",
    "        fixed_point_no_grad_max_iters: int = 0,\n",
    "        fixed_point_with_grad_min_iters: int = 28, \n",
    "        fixed_point_with_grad_max_iters: int = 28,\n",
    "\n",
    "        # solution recycle\n",
    "        fixed_point_reuse_solution = False,\n",
    "        \n",
    "        # pre_post_timestep_conditioning\n",
    "        fixed_point_pre_post_timestep_conditioning: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.learn_sigma = learn_sigma\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = in_channels * 2 if learn_sigma else in_channels\n",
    "        self.patch_size = patch_size\n",
    "        self.num_heads = num_heads\n",
    "        self.use_gradient_checkpointing = use_gradient_checkpointing\n",
    "\n",
    "        self.x_embedder = PatchEmbed(input_size, patch_size, in_channels, hidden_size, bias=True)\n",
    "        self.t_embedder = TimestepEmbedder(hidden_size)\n",
    "        self.y_embedder = LabelEmbedder(num_classes, hidden_size, class_dropout_prob, \n",
    "            use_cfg_embedding=use_cfg_embedding, continuous=is_label_continuous)\n",
    "        num_patches = self.x_embedder.num_patches\n",
    "        \n",
    "        # Will use fixed sin-cos embedding:\n",
    "        # self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, hidden_size), requires_grad=False)\n",
    "        self.register_buffer('pos_embed', torch.zeros(1, num_patches, hidden_size))\n",
    "\n",
    "        # New: Fixed Point\n",
    "        self.fixed_point = fixed_point\n",
    "        if self.fixed_point:\n",
    "            self.fixed_point_no_grad_min_iters = fixed_point_no_grad_min_iters\n",
    "            self.fixed_point_no_grad_max_iters = fixed_point_no_grad_max_iters\n",
    "            self.fixed_point_with_grad_min_iters = fixed_point_with_grad_min_iters\n",
    "            self.fixed_point_with_grad_max_iters = fixed_point_with_grad_max_iters\n",
    "            self.fixed_point_pre_post_timestep_conditioning = fixed_point_pre_post_timestep_conditioning\n",
    "            self.blocks_pre = nn.ModuleList([DiTBlock(hidden_size, num_heads, mlp_ratio=mlp_ratio) for _ in range(fixed_point_pre_depth)])\n",
    "            self.block_pre_projection = nn.Linear(hidden_size, hidden_size)\n",
    "            self.block_fixed_point_projection_fc1 = nn.Linear(2 * hidden_size, 2 * hidden_size)\n",
    "            self.block_fixed_point_projection_act = nn.GELU(approximate=\"tanh\")\n",
    "            self.block_fixed_point_projection_fc2 = nn.Linear(2 * hidden_size, hidden_size)\n",
    "            self.block_fixed_point = DiTBlock(hidden_size, num_heads, mlp_ratio=mlp_ratio)\n",
    "            self.blocks_post = nn.ModuleList([DiTBlock(hidden_size, num_heads, mlp_ratio=mlp_ratio) for _ in range(fixed_point_post_depth)])\n",
    "            self.blocks = [*self.blocks_pre, self.block_fixed_point, *self.blocks_post]\n",
    "            self.fixed_point_reuse_solution = fixed_point_reuse_solution\n",
    "            self.last_solution = None\n",
    "        else:\n",
    "            self.blocks = nn.ModuleList([DiTBlock(hidden_size, num_heads, mlp_ratio=mlp_ratio) for _ in range(depth)])\n",
    "        self.final_layer = FinalLayer(hidden_size, patch_size, self.out_channels)\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # Initialize transformer layers:\n",
    "        def _basic_init(module):\n",
    "            if isinstance(module, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "        self.apply(_basic_init)\n",
    "\n",
    "        # Initialize (and freeze) pos_embed by sin-cos embedding:\n",
    "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.x_embedder.num_patches ** 0.5))\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        # Initialize patch_embed like nn.Linear (instead of nn.Conv2d):\n",
    "        w = self.x_embedder.proj.weight.data\n",
    "        nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
    "        nn.init.constant_(self.x_embedder.proj.bias, 0)\n",
    "\n",
    "        # Initialize label embedding table:\n",
    "        if self.y_embedder.continuous:\n",
    "            nn.init.normal_(self.y_embedder.embedding_projection.weight, std=0.02)\n",
    "            nn.init.constant_(self.y_embedder.embedding_projection.bias, 0)\n",
    "        else:\n",
    "            nn.init.normal_(self.y_embedder.embedding_table.weight, std=0.02)\n",
    "\n",
    "        # Initialize timestep embedding MLP:\n",
    "        nn.init.normal_(self.t_embedder.mlp[0].weight, std=0.02)\n",
    "        nn.init.normal_(self.t_embedder.mlp[2].weight, std=0.02)\n",
    "\n",
    "        # Zero-out adaLN modulation layers in DiT blocks:\n",
    "        for block in self.blocks:\n",
    "            nn.init.constant_(block.adaLN_modulation[-1].weight, 0)\n",
    "            nn.init.constant_(block.adaLN_modulation[-1].bias, 0)\n",
    "\n",
    "        # Zero-out output layers:\n",
    "        nn.init.constant_(self.final_layer.adaLN_modulation[-1].weight, 0)\n",
    "        nn.init.constant_(self.final_layer.adaLN_modulation[-1].bias, 0)\n",
    "        nn.init.constant_(self.final_layer.linear.weight, 0)\n",
    "        nn.init.constant_(self.final_layer.linear.bias, 0)\n",
    "\n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"\n",
    "        Reshapes the patches back to image format.\n",
    "        x: (N, T, patch_size**2 * C)\n",
    "        imgs: (N, H, W, C)\n",
    "        \"\"\"\n",
    "        c = self.out_channels\n",
    "        p = self.x_embedder.patch_size[0]\n",
    "        h = w = int(x.shape[1] ** 0.5)\n",
    "        assert h * w == x.shape[1]\n",
    "\n",
    "        x = x.reshape(shape=(x.shape[0], h, w, p, p, c))\n",
    "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "        imgs = x.reshape(shape=(x.shape[0], c, h * p, h * p))\n",
    "        return imgs\n",
    "    \n",
    "    def ckpt_wrapper(self, module):\n",
    "        \"\"\"\n",
    "        Wrapper function for gradient checkpointing.\n",
    "        \"\"\"\n",
    "        def ckpt_forward(*inputs):\n",
    "            outputs = module(*inputs)\n",
    "            return outputs\n",
    "        return ckpt_forward\n",
    "\n",
    "    def _forward_dit(self, x, t, y):\n",
    "        x = self.x_embedder(x) + self.pos_embed  # (N, T, D), where T = H * W / patch_size ** 2\n",
    "        t = self.t_embedder(t)                   # (N, D)\n",
    "        y = self.y_embedder(y, self.training)    # (N, D))\n",
    "        c = t + y                                # (N, D)\n",
    "        for block in self.blocks:\n",
    "            x = checkpoint(self.ckpt_wrapper(block), x, c) if self.use_gradient_checkpointing else block(x, c)  # (N, T, D)\n",
    "        x = self.final_layer(x, c)                # (N, T, patch_size ** 2 * out_channels)\n",
    "        x = self.unpatchify(x)                   # (N, out_channels, H, W)\n",
    "        return x\n",
    "    \n",
    "    def _forward_fixed_point_blocks(\n",
    "        self, x: Float[Tensor, \"b t d\"], x_input_injection: Float[Tensor, \"b t d\"], c: Float[Tensor, \"b d\"], num_iterations: int\n",
    "    ) -> Float[Tensor, \"b t d\"]:\n",
    "        for _ in range(num_iterations):\n",
    "            x = torch.cat((x, x_input_injection), dim=-1)  # (N, T, D * 2)\n",
    "            x = self.block_fixed_point_projection_fc1(x)  # (N, T, D * 2)\n",
    "            x = self.block_fixed_point_projection_act(x)  # (N, T, D * 2)\n",
    "            x = self.block_fixed_point_projection_fc2(x)  # (N, T, D)\n",
    "            x = self.block_fixed_point(x, c)  # (N, T, D)\n",
    "        return x\n",
    "    \n",
    "    def _check_inputs(self, x: Float[Tensor, \"b c h w\"], t: Shaped[Tensor, \"b\"], y: Shaped[Tensor, \"b\"]) -> None:\n",
    "        if self.fixed_point_reuse_solution:\n",
    "            if not torch.all(t[0] == t).item():\n",
    "                raise ValueError(t)\n",
    "\n",
    "    def _forward_fixed_point(self, x: Float[Tensor, \"b c h w\"], t: Shaped[Tensor, \"b\"], y: Shaped[Tensor, \"b\"]) -> Float[Tensor, \"b c h w\"]:\n",
    "        self._check_inputs(x, t, y)\n",
    "        x: Float[Tensor, \"b t d\"] = self.x_embedder(x) + self.pos_embed\n",
    "        t_emb: Float[Tensor, \"b d\"] = self.t_embedder(t)\n",
    "        y: Float[Tensor, \"b d\"] = self.y_embedder(y, self.training)\n",
    "        c: Float[Tensor, \"b d\"] = t_emb + y\n",
    "        c_pre_post_fixed_point: Float[Tensor, \"b d\"] = (t_emb + y) if self.fixed_point_pre_post_timestep_conditioning else y\n",
    "        \n",
    "        # Pre-Fixed Point\n",
    "        # Note: If using DDP with find_unused_parameters=True, checkpoint causes issues. For more \n",
    "        # information, see https://github.com/allenai/longformer/issues/63#issuecomment-648861503\n",
    "        for block in self.blocks_pre:\n",
    "            x: Float[Tensor, \"b t d\"] = checkpoint(self.ckpt_wrapper(block), x, c_pre_post_fixed_point) if self.use_gradient_checkpointing else block(x, c_pre_post_fixed_point)\n",
    "        condition = x.clone()\n",
    "\n",
    "        # Whether to reuse the previous solution at the next iteration\n",
    "        init_solution = self.last_solution if (self.fixed_point_reuse_solution and self.last_solution is not None) else x.clone()\n",
    "\n",
    "        # Fixed Point (we have condition and init_solution)\n",
    "        x_input_injection = self.block_pre_projection(condition)\n",
    "\n",
    "        # NOTE: This section of code should have no_grad, but cannot due to a DDP bug. See\n",
    "        # https://discuss.pytorch.org/t/does-distributeddataparallel-work-with-torch-no-grad-and-find-unused-parameters-false/122594\n",
    "        # for more information\n",
    "        with nullcontext():  # we use x.detach() in place of torch.no_grad due to DDP issue\n",
    "            num_iterations_no_grad = random.randint(self.fixed_point_no_grad_min_iters, self.fixed_point_no_grad_max_iters)\n",
    "            x = self._forward_fixed_point_blocks(x=init_solution.detach(), x_input_injection=x_input_injection.detach(), c=c, num_iterations=num_iterations_no_grad)\n",
    "            x = x.detach()  # no grad\n",
    "        num_iterations_with_grad = random.randint(self.fixed_point_with_grad_min_iters, self.fixed_point_with_grad_max_iters)\n",
    "        x = self._forward_fixed_point_blocks(x=x, x_input_injection=x_input_injection, c=c, num_iterations=num_iterations_with_grad)\n",
    "\n",
    "        # Save solution for reuse at next step\n",
    "        if self.fixed_point_reuse_solution:\n",
    "            self.last_solution = x.clone()\n",
    "        \n",
    "        # Post-Fixed Point\n",
    "        for block in self.blocks_post:\n",
    "            x = checkpoint(self.ckpt_wrapper(block), x, c_pre_post_fixed_point) if self.use_gradient_checkpointing else block(x, c_pre_post_fixed_point)\n",
    "        \n",
    "        # Output\n",
    "        x: Float[Tensor, \"b t p2c\"] = self.final_layer(x, c_pre_post_fixed_point)  # p2c = patch_size ** 2 * out_channels)\n",
    "        x: Float[Tensor, \"b c h w\"] = self.unpatchify(x)\n",
    "        return x\n",
    "    \n",
    "    def reset(self):\n",
    "        self.last_solution = None\n",
    "    \n",
    "    def forward(self, x, t, y):\n",
    "        \"\"\"\n",
    "        General forward pass method which handles both standard and fixed point modes.\n",
    "        x: (N, C, H, W) tensor of spatial inputs (images or latent representations of images)\n",
    "        t: (N,) tensor of diffusion timesteps\n",
    "        y: (N,) tensor of class labels\n",
    "        \"\"\"\n",
    "        if self.fixed_point:\n",
    "            return self._forward_fixed_point(x, t, y)\n",
    "        else:\n",
    "            return self._forward_dit(x, t, y)\n",
    "\n",
    "    def forward_with_cfg(self, x, t, y, cfg_scale):\n",
    "        \"\"\"\n",
    "        Forward pass of DiT, but also batches the unconditional forward pass for classifier-free guidance.\n",
    "        \"\"\"\n",
    "        # https://github.com/openai/glide-text2im/blob/main/notebooks/text2im.ipynb\n",
    "        half = x[: len(x) // 2]\n",
    "        combined = torch.cat([half, half], dim=0)\n",
    "        model_out = self.forward(combined, t, y)\n",
    "        # For exact reproducibility reasons, we apply classifier-free guidance on only\n",
    "        # three channels by default. The standard approach to cfg applies it to all channels.\n",
    "        # This can be done by uncommenting the following line and commenting-out the line following that.\n",
    "        # eps, rest = model_out[:, :self.in_channels], model_out[:, self.in_channels:]\n",
    "        eps, rest = model_out[:, :3], model_out[:, 3:]\n",
    "        cond_eps, uncond_eps = torch.split(eps, len(eps) // 2, dim=0)\n",
    "        half_eps = uncond_eps + cfg_scale * (cond_eps - uncond_eps)\n",
    "        eps = torch.cat([half_eps, half_eps], dim=0)\n",
    "        return torch.cat([eps, rest], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Accelerator for GPU/CPU acceleration and create the DiT model.\n",
    "accelerator = Accelerator()\n",
    "model = DiT_models[args.model](\n",
    "        input_size=args.latent_size,\n",
    "        num_classes=(args.dino_supervised_dim if args.dino_supervised else args.num_classes),\n",
    "        is_label_continuous=args.dino_supervised,\n",
    "        class_dropout_prob=0,\n",
    "        learn_sigma=(not args.flow),  # TODO: Implement learned variance for flow-based models\n",
    "        use_gradient_checkpointing=False,\n",
    "        fixed_point=args.fixed_point,\n",
    "        fixed_point_pre_depth=args.fixed_point_pre_depth,\n",
    "        fixed_point_post_depth=args.fixed_point_post_depth,\n",
    "        fixed_point_no_grad_min_iters=0, \n",
    "        fixed_point_no_grad_max_iters=0,\n",
    "        fixed_point_with_grad_min_iters=args.fixed_point_iters, \n",
    "        fixed_point_with_grad_max_iters=args.fixed_point_iters,\n",
    "        fixed_point_reuse_solution=args.fixed_point_reuse_solution,\n",
    "        fixed_point_pre_post_timestep_conditioning=args.fixed_point_pre_post_timestep_conditioning,\n",
    "    ).to(accelerator.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model checkpoint.\n",
    "state_dict = find_model(args.ckpt)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval() \n",
    "\n",
    "# Initialize the diffusion process with specified parameters.\n",
    "diffusion = create_diffusion(\n",
    "    str(args.num_sampling_steps), \n",
    "    use_flow=args.flow,\n",
    "    predict_v=args.predict_v,\n",
    "    use_zero_terminal_snr=args.use_zero_terminal_snr,\n",
    ")\n",
    "\n",
    "# Load the VAE model and evaluate it.\n",
    "vae = AutoencoderKL.from_pretrained(f\"stabilityai/sd-vae-ft-{args.vae}\").to(accelerator.device).eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create generator, class labels, and latents\n",
    "N_images = 32\n",
    "generator = torch.Generator(device=accelerator.device)\n",
    "generator.manual_seed(args.global_seed)\n",
    "class_labels = torch.randint(0, args.num_classes, size=(N_images,), device=accelerator.device)\n",
    "generator.manual_seed(args.global_seed)\n",
    "latents = torch.randn(N_images, model.in_channels, args.H_lat, args.W_lat, device=accelerator.device, generator=generator)\n",
    "class_labels = class_labels[args.sample_index_start:args.sample_index_end]\n",
    "latents = latents[args.sample_index_start:args.sample_index_end]\n",
    "indices = list(range(args.sample_index_start, args.sample_index_end))\n",
    "print(f'Using pseudorandom class labels and latents (start={args.sample_index_start} and end={args.sample_index_end})')\n",
    "\n",
    " # Create output path\n",
    "output_dir = Path(args.output_dir)\n",
    "# if cfg is used\n",
    "using_cfg = args.cfg_scale > 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load class labels for helpful filenames\n",
    "if args.dataset_name == 'imagenet256':\n",
    "    with open(\"../utils/imagenet-labels.json\", \"r\") as f:\n",
    "        label_names: list[str] = json.load(f)\n",
    "        label_names = [l.lower().replace(' ', '-').replace('\\'', '') for l in label_names]\n",
    "elif args.unsupervised:\n",
    "    assert args.cfg_scale == 1.0\n",
    "    label_names = [\"unlabeled\"]\n",
    "else:\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    # Sample loop\n",
    "    num_batches = math.ceil(len(class_labels) / args.batch_size)\n",
    "    for batch_idx in trange(num_batches, disable=(not accelerator.is_main_process)):\n",
    "\n",
    "        # Get pre-sampled inputs\n",
    "        z = latents[batch_idx*args.batch_size:(batch_idx + 1)*args.batch_size]\n",
    "        y = class_labels[batch_idx*args.batch_size:(batch_idx + 1)*args.batch_size]\n",
    "        idxs = indices[batch_idx*args.batch_size:(batch_idx + 1)*args.batch_size]\n",
    "        output_paths = [output_dir / f'{idx:05d}--{y_i:03d}--{label_names[y_i]}.png' for y_i, idx in zip(y.tolist(), idxs)]\n",
    "\n",
    "        # Skip files that already exist\n",
    "        if all(output_path.is_file() for output_path in output_paths):\n",
    "            print(f'Files already exist (batch {batch_idx}). Skipping.')\n",
    "            continue\n",
    "\n",
    "        # Setup classifier-free guidance\n",
    "        if using_cfg:\n",
    "            y_null = torch.tensor([1000] * args.batch_size, device=accelerator.device)\n",
    "            y = torch.cat([y, y_null], 0)\n",
    "            z = torch.cat([z, z], 0)\n",
    "            model_kwargs = dict(y=y, cfg_scale=args.cfg_scale)\n",
    "            sample_fn = model.forward_with_cfg\n",
    "        else:\n",
    "            model_kwargs = dict(y=y)\n",
    "            sample_fn = model.forward\n",
    "\n",
    "        # Sample latent images\n",
    "        sample_kwargs = dict(model=sample_fn, shape=z.shape, noise=z, clip_denoised=False, model_kwargs=model_kwargs, \n",
    "            progress=False, device=accelerator.device)\n",
    "        if args.ddim:\n",
    "            samples = diffusion.ddim_sample_loop(**sample_kwargs)\n",
    "        else:\n",
    "            samples = diffusion.p_sample_loop(**sample_kwargs)\n",
    "\n",
    "        if using_cfg:\n",
    "            samples, _ = samples.chunk(2, dim=0)\n",
    "        \n",
    "        # Reset model (resets the initial solution to None)\n",
    "        model.reset()\n",
    "\n",
    "        # Decode latents\n",
    "        samples = vae.decode(samples / vae.config.scaling_factor).sample\n",
    "        samples = torch.clamp(127.5 * samples + 128.0, 0, 255).permute(0, 2, 3, 1).to(\"cpu\", dtype=torch.uint8).numpy()\n",
    "\n",
    "        # Save samples to disk as individual .png files\n",
    "        for sample, output_path in zip(samples, output_paths):\n",
    "            Image.fromarray(sample).save(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# get all files in output_dir\n",
    "output_dir = Path(args.output_dir)\n",
    "files = list(output_dir.glob('*.png'))\n",
    "\n",
    "# visualize four random samples\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "for ax in axs.flatten():\n",
    "    img = Image.open(files[random.randint(0, len(files) - 1)])\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "renaissance3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
